{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeab4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 20.25 MiB is free. Including non-PyTorch memory, this process has 1.92 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 46.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m tokenizer = CLIPTokenizer(\u001b[33m\"\u001b[39m\u001b[33m../data/vocab.json\u001b[39m\u001b[33m\"\u001b[39m, merges_file=\u001b[33m\"\u001b[39m\u001b[33m../data/tokenizer_merges.txt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m model_file = \u001b[33m\"\u001b[39m\u001b[33m../data/v1-5-pruned-emaonly.ckpt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m models = \u001b[43mmodel_loader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpreload_models_from_standard_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/TorchLeet/diffusion/stable_diffusion/model_loader.py:9\u001b[39m, in \u001b[36mpreload_models_from_standard_weights\u001b[39m\u001b[34m(ckpt_path, device)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpreload_models_from_standard_weights\u001b[39m(ckpt_path, device):\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     state_dict = \u001b[43mmodel_converter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_from_standard_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     encoder = VAE_Encoder().to(device)\n\u001b[32m     12\u001b[39m     encoder.load_state_dict(state_dict=[\u001b[33m\"\u001b[39m\u001b[33mencoder\u001b[39m\u001b[33m\"\u001b[39m], strict=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/TorchLeet/diffusion/stable_diffusion/model_converter.py:5\u001b[39m, in \u001b[36mload_from_standard_weights\u001b[39m\u001b[34m(input_file, device)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_from_standard_weights\u001b[39m(input_file: \u001b[38;5;28mstr\u001b[39m, device: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, torch.Tensor]:\n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m# Taken from: https://github.com/kjsman/stable-diffusion-pytorch/issues/7#issuecomment-1426839447\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     original_model = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mstate_dict\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      7\u001b[39m     converted = {}\n\u001b[32m      8\u001b[39m     converted[\u001b[33m'\u001b[39m\u001b[33mdiffusion\u001b[39m\u001b[33m'\u001b[39m] = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torchleet/lib/python3.13/site-packages/torch/serialization.py:1360\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1358\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1359\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1360\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1362\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1364\u001b[39m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1366\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1367\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[32m   1368\u001b[39m     f_name = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torchleet/lib/python3.13/site-packages/torch/serialization.py:1848\u001b[39m, in \u001b[36m_load\u001b[39m\u001b[34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[39m\n\u001b[32m   1846\u001b[39m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[32m   1847\u001b[39m _serialization_tls.map_location = map_location\n\u001b[32m-> \u001b[39m\u001b[32m1848\u001b[39m result = \u001b[43munpickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1849\u001b[39m _serialization_tls.map_location = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1851\u001b[39m torch._utils._validate_loaded_sparse_tensors()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torchleet/lib/python3.13/site-packages/torch/serialization.py:1812\u001b[39m, in \u001b[36m_load.<locals>.persistent_load\u001b[39m\u001b[34m(saved_id)\u001b[39m\n\u001b[32m   1810\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1811\u001b[39m     nbytes = numel * torch._utils._element_size(dtype)\n\u001b[32m-> \u001b[39m\u001b[32m1812\u001b[39m     typed_storage = \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1813\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1814\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1816\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torchleet/lib/python3.13/site-packages/torch/serialization.py:1784\u001b[39m, in \u001b[36m_load.<locals>.load_tensor\u001b[39m\u001b[34m(dtype, numel, key, location)\u001b[39m\n\u001b[32m   1779\u001b[39m         storage.byteswap(dtype)\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[32m   1783\u001b[39m typed_storage = torch.storage.TypedStorage(\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     wrap_storage=\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1785\u001b[39m     dtype=dtype,\n\u001b[32m   1786\u001b[39m     _internal=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   1787\u001b[39m )\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typed_storage._data_ptr() != \u001b[32m0\u001b[39m:\n\u001b[32m   1790\u001b[39m     loaded_storages[key] = typed_storage\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torchleet/lib/python3.13/site-packages/torch/serialization.py:1685\u001b[39m, in \u001b[36m_get_restore_location.<locals>.restore_location\u001b[39m\u001b[34m(storage, location)\u001b[39m\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrestore_location\u001b[39m(storage, location):\n\u001b[32m-> \u001b[39m\u001b[32m1685\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdefault_restore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torchleet/lib/python3.13/site-packages/torch/serialization.py:601\u001b[39m, in \u001b[36mdefault_restore_location\u001b[39m\u001b[34m(storage, location)\u001b[39m\n\u001b[32m    581\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    582\u001b[39m \u001b[33;03mRestores `storage` using a deserializer function registered for the `location`.\u001b[39;00m\n\u001b[32m    583\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    598\u001b[39m \u001b[33;03m       all matching ones return `None`.\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    600\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[32m--> \u001b[39m\u001b[32m601\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    603\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torchleet/lib/python3.13/site-packages/torch/serialization.py:540\u001b[39m, in \u001b[36m_deserialize\u001b[39m\u001b[34m(backend_name, obj, location)\u001b[39m\n\u001b[32m    538\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m location.startswith(backend_name):\n\u001b[32m    539\u001b[39m     device = _validate_device(location, backend_name)\n\u001b[32m--> \u001b[39m\u001b[32m540\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torchleet/lib/python3.13/site-packages/torch/storage.py:279\u001b[39m, in \u001b[36m_StorageBase.to\u001b[39m\u001b[34m(self, device, non_blocking)\u001b[39m\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mto\u001b[39m(\n\u001b[32m    277\u001b[39m     \u001b[38;5;28mself\u001b[39m, *, device: torch.device, non_blocking: _bool = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    278\u001b[39m ) -> Union[_StorageBase, TypedStorage]:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_to\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torchleet/lib/python3.13/site-packages/torch/_utils.py:88\u001b[39m, in \u001b[36m_to\u001b[39m\u001b[34m(self, device, non_blocking)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     85\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m     86\u001b[39m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_sparse\n\u001b[32m     87\u001b[39m     ), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33msparse storage is not supported for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice.type.upper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m tensors\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     untyped_storage = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m     untyped_storage.copy_(\u001b[38;5;28mself\u001b[39m, non_blocking)\n\u001b[32m     90\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m untyped_storage\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 20.25 MiB is free. Including non-PyTorch memory, this process has 1.92 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 46.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import model_loader\n",
    "import pipeline\n",
    "from PIL import Image\n",
    "from transformers import CLIPTokenizer\n",
    "import torch\n",
    "\n",
    "ALLOW_CUDA = False\n",
    "ALLOW_MPS = True\n",
    "\n",
    "if torch.cuda.is_available() and ALLOW_CUDA:\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available() and ALLOW_MPS:\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "tokenizer = CLIPTokenizer(\"../data/vocab.json\", merges_file=\"../data/tokenizer_merges.txt\")\n",
    "model_file = \"../data/v1-5-pruned-emaonly.ckpt\"\n",
    "models = model_loader.preload_models_from_standard_weights(model_file, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc97acd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXT TO IMAGE\n",
    "\n",
    "prompt = \"A street lamp\"\n",
    "uncond_prompt = \"A table lamp\"\n",
    "do_cfg = True\n",
    "cfg_scale = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d3dd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGE to IMAGE\n",
    "\n",
    "input_image = None\n",
    "image_path = \"../data/dog.jpg\"\n",
    "strength = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaa54a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = \"ddpm\"\n",
    "num_inference_steps = 50\n",
    "seed = 42\n",
    "\n",
    "output_image = pipeline.generate(\n",
    "    prompt=prompt,\n",
    "    uncond_prompt=uncond_prompt,\n",
    "    input_image=input_image,\n",
    "    do_cfg=do_cfg,\n",
    "    cfg_scale=cfg_scale,\n",
    "    sampler_name=sampler,\n",
    "    n_inference_steps=num_inference_steps,\n",
    "    seed=seed,\n",
    "    models=models,\n",
    "    device=device,\n",
    "    idle_device=\"cpu\",\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "Image.fromarray(output_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchleet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
